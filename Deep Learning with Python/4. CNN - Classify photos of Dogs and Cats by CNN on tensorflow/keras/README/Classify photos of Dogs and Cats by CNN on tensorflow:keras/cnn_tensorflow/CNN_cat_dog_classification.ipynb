{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDg326adoF_U"
   },
   "source": [
    "# Classify Cat-Dog images using  CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-JHfjD4oF_V"
   },
   "source": [
    "In this notebook, we classify cat/dog images by using CNN model.\n",
    "\n",
    "Architecture of our network is:\n",
    "    \n",
    "- (Input) -> [batch_size, 64, 64, 3]  >> Apply 32 filter of [5x5]\n",
    "- (Convolutional layer 1)  -> [batch_size, 64, 64, 32]\n",
    "- (ReLU 1)  -> [?, 64, 64, 32]\n",
    "- (Max pooling 1) -> [?, 32, 32, 32]\n",
    "- (Convolutional layer 2)  -> [?, 32, 32, 64] \n",
    "- (ReLU 2)  -> [?, 32, 32, 64] \n",
    "- (Max pooling 2)  -> [?, 16, 16, 64] \n",
    "- [fully connected layer 3] -> [1x1024]\n",
    "- [ReLU 3]  -> [1x1024]\n",
    "- [Drop out]  -> [1x1024]\n",
    "- [fully connected layer 4] -> [1x1]\n",
    "\n",
    "\n",
    "The next cells will explore this new architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "-lwQb5hwoQ7E",
    "outputId": "ff4f3ecf-7634-4bf6-a1b1-0bee70170f08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "M9RA2AzL3b8_",
    "outputId": "6b9ad77b-2fd3-40ef-abf8-d2984e19668b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'20190527_162723 (1).jpg'  '20190527_165048 (1).jpg'\n",
      " 20190527_162723.jpg\t    20190527_165048.jpg\n",
      "'20190527_162753 (1).jpg'   20190527_171116.jpg\n",
      " 20190527_162753.jpg\t   '20190527_171656 (1).jpg'\n",
      "'20190527_162801 (1).jpg'   20190527_171656.jpg\n",
      " 20190527_162801.jpg\t   'Colab Notebooks'\n",
      "'20190527_162805 (1).jpg'   GeC.Debut.pdf\n",
      " 20190527_162805.jpg\t    ORG_DSC09353.JPG\n",
      "'20190527_162811 (1).jpg'   ORG_DSC09357.JPG\n",
      " 20190527_162811.jpg\t    ORG_DSC09363.JPG\n",
      "'20190527_162825 (1).jpg'   ORG_DSC09365.JPG\n",
      " 20190527_162825.jpg\t    pythondatasciencehandbook.pdf\n",
      "'20190527_162845 (1).jpg'  'Rendez-vous Entretien PPAE 20190507.pdf'\n",
      " 20190527_162845.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/gdrive/My Drive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYKBXzNsoF_W"
   },
   "source": [
    "### Starting the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xkVBis9coF_W"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2                 # working with, mainly resizing, images\n",
    "import numpy as np         # dealing with arrays\n",
    "import os                  # dealing with directories\n",
    "from random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n",
    "from tqdm import tqdm      # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion\n",
    "import matplotlib.pyplot as plt\n",
    "#Start interaction session\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhKJm1-doF_a"
   },
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nu18me60oF_b"
   },
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = 64\n",
    "TRAIN_DIR= 'dataset/training_set'\n",
    "TEST_DIR = 'dataset/test_set'\n",
    "LR = 1e-3\n",
    "\n",
    "MODEL_NAME = 'dogsvscats-{}-{}.model'.format(LR, '2conv-basic') # just so we remember which saved model is which, sizes must match\n",
    "\n",
    "# import os\n",
    "# data_dir = os.path.expanduser('dataset')\n",
    "\n",
    "# train_data_dir = os.path.join(data_dir, 'training_set')\n",
    "# test_data_dir = os.path.join(data_dir, 'test_set')\n",
    "# #validation_data_dir = path.join(data_dir, 'validation')\n",
    "\n",
    "# class_names = os.listdir(train_data_dir) # Get names of classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfC-ZlBloF_d"
   },
   "source": [
    "#### Write function to label our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJLBZ3Y_oF_e"
   },
   "outputs": [],
   "source": [
    "def label_img(img):\n",
    "    word_label = img.split('.')[0]\n",
    "    # conversion to one-hot array [cat,dog]\n",
    "    if word_label == 'cat':\n",
    "        return [1,0]\n",
    "    elif word_label == 'dog':\n",
    "        return [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVeoJ9YBoF_h"
   },
   "source": [
    "#### Now we build another function to process the training and test images and their labels into arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kIl0nJdoF_h"
   },
   "outputs": [],
   "source": [
    "def create_data(IMG_SIZE,DIR):\n",
    "    data = []\n",
    "    for img in tqdm(os.listdir(DIR)):\n",
    "        label = label_img(img)\n",
    "        path = os.path.join(DIR,img)\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        data.append([np.array(img)/225.,np.array(label)])\n",
    "    shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCXYE_dGoF_j"
   },
   "source": [
    "_If your directory is a list (dogs and cats images in two separated folder), use the code below:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QEWOzzkOoF_k"
   },
   "outputs": [],
   "source": [
    "#TRAIN_DIR= ['dataset/training_set/dogs','dataset/training_set/cats']\n",
    "#TEST_DIR = ['dataset/test_set/dogs','dataset/test_set/cats']\n",
    "\n",
    "# def create_train_data(IMG_SIZE):\n",
    "#     training_data =[]\n",
    "#     for directory in tqdm(TRAIN_DIR):\n",
    "#         for img in tqdm(os.listdir(directory)):\n",
    "#             label = label_img(img)\n",
    "#             path = os.path.join(directory,img)\n",
    "#             img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "#             img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n",
    "#             training_data.append([np.array(img),np.array(label)])\n",
    "#     shuffle(training_data)\n",
    "#     np.save('train_data.npy',training_data)\n",
    "#     return training_data\n",
    "# #see more: https://pythonprogramming.net/convolutional-neural-network-kats-vs-dogs-machine-learning-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EET9mheioF_m"
   },
   "source": [
    "Create a training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYNRFKW4oF_n",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training_data = create_data(IMG_SIZE,TRAIN_DIR)   \n",
    "# np.save('Colab Notebooks/dataset/train_data.npy', training_data)\n",
    "training_data = np.load('./gdrive/My Drive/Colab Notebooks/CNN/dataset/train_data.npy',allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0xXa5vUoF_p"
   },
   "source": [
    "Create a test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "al3viVkUoF_q"
   },
   "outputs": [],
   "source": [
    "# test_data = create_data(IMG_SIZE,TEST_DIR)   \n",
    "# np.save('test_data.npy', test_data)\n",
    "test_data = np.load('./gdrive/My Drive/Colab Notebooks/CNN/dataset/test_data.npy',allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDP1qvA6oF_s"
   },
   "source": [
    "_For the images which are not labeled, we build the same function to process them but not contain labels._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Of_eqmdOoF_t"
   },
   "outputs": [],
   "source": [
    "# def process_unlabeled_data(IMG_SIZE,DIR):\n",
    "#     data = []\n",
    "#     for img in tqdm(os.listdir(DIR)):\n",
    "#         im_num = img.split('.')[0]\n",
    "#         path = os.path.join(DIR,img)\n",
    "#         img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "#         img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "#         training_data.append([np.array(img)/225.,im_num])\n",
    "#     shuffle(data)\n",
    "#     return data\n",
    "# #Create a unlabeled dataset:\n",
    "# unlabeled_data = process_unlabeled_data(64,UNLABELED_DIR)\n",
    "# np.save('unlabeled_data.npy', unlabeled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edQuP3GMoF_v"
   },
   "source": [
    "#### Converting images of the data set to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPKshNbWoF_w"
   },
   "outputs": [],
   "source": [
    "#Training set\n",
    "X_train = np.array([i[0] for i in training_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "y_train = [i[1] for i in training_data]\n",
    "#Test set\n",
    "X_test =  np.array([i[0] for i in test_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "y_test = [i[1] for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5MV_2JI4oF_z",
    "outputId": "d3057f35-009a-4a21-da24-b17c40bee0d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRnd14A-oF_2"
   },
   "source": [
    "### Initial parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yY2mpejMoF_3"
   },
   "source": [
    "Create general parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qlp4belPoF_4"
   },
   "outputs": [],
   "source": [
    "width = 64 #witdh of the image in pixels\n",
    "height = 64 #height of the image in pixels\n",
    "n_channels = 1 # image channels (3 for colorscale and 1 for grayscale).\n",
    "class_output = 2 #number of possible classifications for the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_gLdM9HoF_6"
   },
   "source": [
    "### Input and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LC4dP33eoF_7"
   },
   "source": [
    "Create place holders for inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xxH09KmoF_7"
   },
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(tf.float32,shape = [None,height,width,n_channels])\n",
    "y_act = tf.placeholder(tf.float32,shape = [None, class_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gsR75y9oF_9"
   },
   "source": [
    "_1st_ dimension: batch number (can be any size),\n",
    "\n",
    "_2nd_ dimension: height,\n",
    "\n",
    "_3rd_ dimension: width,\n",
    "\n",
    "_4th_ dimension: image channels (3 channels as colorscale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJdpxa92oF_-"
   },
   "source": [
    "### 2. Convolutional Layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Hif4MaaoF_-"
   },
   "source": [
    "<h4>Defining kernel weight and bias</h4>\n",
    "We define a kernel here. The Size of the filter/kernel is 5x5;  Input channels is 1 (grayscale);  and we need 32 different feature maps (here, 32 feature maps means 32 different filters are applied on each image. So, the output of convolution layer would be 64x64x32). In this step, we create a filter / kernel tensor of shape <code>[filter_height, filter_width, in_channels, out_channels]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSjtxG7qoF__"
   },
   "outputs": [],
   "source": [
    "W_conv1 = tf.Variable(tf.truncated_normal([5,5,1,32], stddev = 0.1 ))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape = [32] )) # 32 biases for 32 outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eeiu2w2aoGAC"
   },
   "source": [
    "<h4>Convolve image with weight tensor and add biases.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yeS3qtooGAC"
   },
   "outputs": [],
   "source": [
    "convolve1= tf.nn.conv2d(x_input, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OuebaKfjoGAF",
    "outputId": "f4a5a860-a1de-44ed-bdb1-f2cf6712ed04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(?, 64, 64, 32) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IazoXsTloGAO"
   },
   "source": [
    "#### Apply the ReLu activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZ9FnA1soGAP"
   },
   "source": [
    "Let $f(x)$ is a ReLU function,\n",
    "$$f(x)=\\max(0,x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXyqE0GloGAS"
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(convolve1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_o4NzIKVoGAU"
   },
   "source": [
    "#### Apply the max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jC26gyIAoGAV",
    "outputId": "296578f8-1947-400a-f52d-c439833aae82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool:0' shape=(?, 32, 32, 32) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = tf.nn.max_pool(h_conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')#max_pool 2x2\n",
    "conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hd9IxaNeoGAX"
   },
   "source": [
    "First layer completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8jIMXrjiLGgc"
   },
   "source": [
    "First layer completed. The output of layer 1  is 32 matrices of [32x32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIok0guvLGgh"
   },
   "source": [
    "### 3. Convolutional Layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxCj1zgyLGgj"
   },
   "source": [
    "#### Weights and Biases of kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "giIQ35eQLGgk"
   },
   "source": [
    "We apply the convolution again in this layer. Lets look at the second layer kernel:  \n",
    "- Filter/kernel: 5x5 (25 pixels) \n",
    "- Input channels: 32 (from the 2nd Conv layer, we had 32 feature maps) \n",
    "- 64 output feature maps  \n",
    "\n",
    "<b>Notice:</b> here, the input image is [32x32x32], the filter is [5x5x32], we use 64 filters of size [5x5x32], and the output of the convolutional layer would be 64 convolved image, [32x32x64].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmlXOr-CLGgl"
   },
   "outputs": [],
   "source": [
    "W_conv2 = tf.Variable(tf.truncated_normal([5,5,32,64], stddev = 0.1 ))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape = [64] )) # 64 biases for 64 outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwqKf-RLLGgo"
   },
   "source": [
    "<h4>Convolve image with weight tensor and add biases.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fYmGAQOLGgp"
   },
   "outputs": [],
   "source": [
    "convolve2= tf.nn.conv2d(conv1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BnOwroCvLGgs",
    "outputId": "06787278-801e-4195-9741-938d7ddf59b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=(?, 32, 32, 64) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BQ44bADLGgu"
   },
   "source": [
    "#### Apply the ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OU2zVpfLGgv"
   },
   "outputs": [],
   "source": [
    "h_conv2 = tf.nn.relu(convolve2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VM61obRkLGgx"
   },
   "source": [
    "#### Apply the max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xTY8cK06LGgx",
    "outputId": "0f7ccd1b-28c0-4718-8785-7599739f9a44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_1:0' shape=(?, 16, 16, 64) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2 = tf.nn.max_pool(h_conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')#max_pool 2x2\n",
    "conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4E9Jx2aOUqF"
   },
   "source": [
    "Second layer completed. The output of layer 2  is 64 matrices of [16x16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5L1K9cXkWUq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4I15Fn7kWSW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGHcevY8kelP"
   },
   "source": [
    "### 4. Convolutional Layer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZNmFklBkelU"
   },
   "source": [
    "#### Weights and Biases of kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6WF0PNw3kelW"
   },
   "source": [
    "We apply the convolution again in this layer. Lets look at the second layer kernel:  \n",
    "- Filter/kernel: 5x5 (25 pixels) \n",
    "- Input channels: 128 (from the 3rd Conv layer, we had 128 feature maps) \n",
    "- 64 output feature maps  \n",
    "\n",
    "<b>Notice:</b> here, the input image is [8x8x128], the filter is [5x5x128], we use 64 filters of size [5x5x128], and the output of the convolutional layer would be 64 convolved image, [8x8x64].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uilPkUckelX"
   },
   "outputs": [],
   "source": [
    "W_conv3 = tf.Variable(tf.truncated_normal([5,5,64,64], stddev = 0.1 ))\n",
    "b_conv3 = tf.Variable(tf.constant(0.1, shape = [64] )) # 64 biases for 64 outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENTbJZoNkela"
   },
   "source": [
    "<h4>Convolve image with weight tensor and add biases.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ozV-cM-Dkelb"
   },
   "outputs": [],
   "source": [
    "convolve3= tf.nn.conv2d(conv2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BYZW5lSLkeld",
    "outputId": "7bda02bd-51ed-479a-98d8-4bdb21d910ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_3:0' shape=(?, 16, 16, 64) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08akjIr9kelg"
   },
   "source": [
    "#### Apply the ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pYY6y0Wkelh"
   },
   "outputs": [],
   "source": [
    "h_conv3 = tf.nn.relu(convolve3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prjujUoykelj"
   },
   "source": [
    "#### Apply the max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QVcccfFWkelk",
    "outputId": "35c1b650-2479-4e49-d9f6-dfa9fe4abe2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_2:0' shape=(?, 8, 8, 64) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3 = tf.nn.max_pool(h_conv3, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')#max_pool 2x2\n",
    "conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Anydbo2wkWQb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SS3kIm8SkWOp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMgRBoGbkWMi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUkbaLGZkWIg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxcF6g-eoGAY"
   },
   "source": [
    "### 5. Convolutional Layer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06vPWi10oGAZ"
   },
   "source": [
    "#### Weights and Biases of kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBsbaSgOoGAZ"
   },
   "source": [
    "We apply the convolution again in this layer. Lets look at the second layer kernel:  \n",
    "- Filter/kernel: 5x5 (25 pixels) \n",
    "- Input channels: 64 (from the 2nd Conv layer, we had 64 feature maps) \n",
    "- 128 output feature maps  \n",
    "\n",
    "<b>Notice:</b> here, the input image is [32x32x64], the filter is [5x5x64], we use 128 filters of size [5x5x64], and the output of the convolutional layer would be 128 convolved image, [32x32x128].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KekMnDARoGAa"
   },
   "outputs": [],
   "source": [
    "W_conv4 = tf.Variable(tf.truncated_normal([5,5,64,128], stddev = 0.1 ))\n",
    "b_conv4 = tf.Variable(tf.constant(0.1, shape = [128] )) # 128 biases for 128 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2noJBmr3oGAc"
   },
   "source": [
    "<h4>Convolve image with weight tensor and add biases.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-qaknxpoGAd"
   },
   "outputs": [],
   "source": [
    "convolve4= tf.nn.conv2d(conv3, W_conv4, strides=[1, 1, 1, 1], padding='SAME') + b_conv4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u_dLGG8hoGAf",
    "outputId": "7810f4db-3881-4732-be8f-25cd5fae7d38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_5:0' shape=(?, 8, 8, 128) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ugeJMtz8oGAi"
   },
   "source": [
    "#### Apply the ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7zC_K9yoGAj"
   },
   "outputs": [],
   "source": [
    "h_conv4 = tf.nn.relu(convolve4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VHXiGfryoGAl"
   },
   "source": [
    "#### Apply the max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z7cYdw0eoGAm",
    "outputId": "ae2e00ac-d6a5-415e-e1b2-623d543521fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_3:0' shape=(?, 4, 4, 128) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv4 = tf.nn.max_pool(h_conv4, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')#max_pool 2x2\n",
    "conv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7vCi92EVoGAp"
   },
   "source": [
    "Third layer completed. The output of layer 3  is 128 matrices of [8x8]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lui2K2vHCypz"
   },
   "source": [
    "### 6. Convolutional Layer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpoE-zQrCyp2"
   },
   "source": [
    "#### Weights and Biases of kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8D2F3GJCyp3"
   },
   "source": [
    "We apply the convolution again in this layer. Lets look at the second layer kernel:  \n",
    "- Filter/kernel: 5x5 (25 pixels) \n",
    "- Input channels: 128 (from the 3rd Conv layer, we had 128 feature maps) \n",
    "- 64 output feature maps  \n",
    "\n",
    "<b>Notice:</b> here, the input image is [8x8x128], the filter is [5x5x128], we use 64 filters of size [5x5x128], and the output of the convolutional layer would be 64 convolved image, [8x8x64].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxwgQsTrCyp5"
   },
   "outputs": [],
   "source": [
    "W_conv5 = tf.Variable(tf.truncated_normal([3,3,128,256], stddev = 0.1 ))\n",
    "b_conv5 = tf.Variable(tf.constant(0.1, shape = [256] )) # 64 biases for 64 outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3g8a55HCyp9"
   },
   "source": [
    "<h4>Convolve image with weight tensor and add biases.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seSOZmReCyp-"
   },
   "outputs": [],
   "source": [
    "convolve5= tf.nn.conv2d(conv4, W_conv5, strides=[1, 1, 1, 1], padding='SAME') + b_conv5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YqhkHTzaCyqC",
    "outputId": "883e763f-96e8-415d-d9bd-2c479a301c79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_7:0' shape=(?, 4, 4, 256) dtype=float32>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I94QziD_CyqE"
   },
   "source": [
    "#### Apply the ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNq8UHOjCyqG"
   },
   "outputs": [],
   "source": [
    "h_conv5 = tf.nn.relu(convolve5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6Xm9vdbCyqI"
   },
   "source": [
    "#### Apply the max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ntrRhaSGCyqI",
    "outputId": "860a233c-e4d6-4b98-8f4c-d04c96de5644"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_4:0' shape=(?, 2, 2, 256) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv5 = tf.nn.max_pool(h_conv5, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')#max_pool 2x2\n",
    "conv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpSg9jF9oGAq"
   },
   "source": [
    "### 7. Fully Connected Layer (Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzPoWblroGAq"
   },
   "source": [
    "You need a fully connected layer to use the Softmax and create the probabilities in the end. Fully connected layers take the high-level filtered images from previous layer, that is all 64 matrices, and convert them to a flat array.\n",
    "\n",
    "So, each matrix [7x7] will be converted to a matrix of [49x1], and then all of the 64 matrix will be connected, which make an array of size [3136x1]. We will connect it into another layer of size [1024x1]. So, the weight between these 2 layers will be [3136x1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqhVOYfkoGAr"
   },
   "source": [
    "#### Flattening Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wEqVoYJmoGAr"
   },
   "outputs": [],
   "source": [
    "layer5_matrix = tf.reshape(conv5,[-1,2*2*256]) # we don't care the 1st dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjG9ZajJoGAt"
   },
   "source": [
    "#### Weights and biase between layer 5 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NYAnoTKgoGAu"
   },
   "outputs": [],
   "source": [
    "W_fc1 = tf.Variable(tf.truncated_normal([2*2*256,1024], stddev = 0.1 ))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape = [1024] )) # 1024 biases for 1024 outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRnbpAEtoGAx"
   },
   "source": [
    "#### Matrix Multiplication (Applying weights and biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_oH4MHDhoGAx"
   },
   "outputs": [],
   "source": [
    "fc1 = tf.matmul(layer5_matrix, W_fc1)+b_fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zR9oQf1KoGA2"
   },
   "source": [
    "#### Apply the ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YmeKmIkioGA3",
    "outputId": "c86bdeac-0283-475c-e322-48751e66468a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_6:0' shape=(?, 1024) dtype=float32>"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_fc1 = tf.nn.relu(fc1)\n",
    "h_fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82AepDHBoGA8"
   },
   "source": [
    "Third layer is completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZbA3xLRoGA9"
   },
   "source": [
    "### 8. Dropout Layer, (Optional phase for reducing overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fP861GRRoGA-"
   },
   "source": [
    "It is a phase where the network \"forget\" some features. At each training step in a mini-batch, some units get switched off randomly so that it will not interact with the network. That is, it weights cannot be updated, nor affect the learning of the other network nodes.  This can be very useful for very large neural networks to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9TEaTr8YoGA_",
    "outputId": "d3e65aa5-d822-4951-9f73-009e3b178ae4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout/mul_1:0' shape=(?, 1024) dtype=float32>"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "layer_drop = tf.nn.dropout(h_fc1, rate = 1-keep_prob)\n",
    "layer_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A99RxjK_oGBB"
   },
   "source": [
    "Note: p (keep_prob)=0.5 is recommended configuration, except for the input layer which is recommended to have p=0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SgEFreSoGBE"
   },
   "source": [
    "In other words, at testing time we treat it as a normal neural network without dropout, and at training time we upscale the values by 1/prob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGLTZXcloGBG"
   },
   "source": [
    "### 9. Readout Layer (Softmax Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2wbKhSvoGBH"
   },
   "source": [
    "Type: Softmax, Fully Connected Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgDkV5IXoGBI"
   },
   "source": [
    "<h4>Weights and Biases</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CAGCSKpoGBJ"
   },
   "source": [
    "In last layer, CNN takes the high-level filtered images and translate them into votes using softmax.\n",
    "Input channels: 1024 (neurons from the 3rd Layer); 10 output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bq20Mt9BoGBK"
   },
   "outputs": [],
   "source": [
    "W_fc2 = tf.Variable(tf.truncated_normal([1024,2], stddev = 0.1 ))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape = [2] )) # 2 biases for 2 output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niIXslXkoGBO"
   },
   "source": [
    "#### Matrix Multiplication (Applying weights and biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "756owgeRoGBR"
   },
   "outputs": [],
   "source": [
    "fc = tf.matmul(layer_drop, W_fc2)+b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHjW3Rz5oGBV"
   },
   "source": [
    "#### Apply the Softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W9V3kN4aoGBW"
   },
   "outputs": [],
   "source": [
    "y_CNN = tf.nn.softmax(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCqkuFHuoGBX"
   },
   "source": [
    "<a id=\"ref7\"></a>\n",
    "<h2>Summary of the Deep Convolutional Neural Network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLh_SvznoGBY"
   },
   "source": [
    "Now is time to remember the structure of  our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RQvPXwDoGBY"
   },
   "source": [
    "#### 0) Input - MNIST dataset\n",
    "#### 1) Convolutional and Max-Pooling\n",
    "#### 2) Convolutional and Max-Pooling\n",
    "#### 3) Fully Connected Layer\n",
    "#### 4) Processing - Dropout\n",
    "#### 5) Readout layer - Fully Connected\n",
    "#### 6) Outputs - Classified digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xralTRGuoGBZ"
   },
   "source": [
    "We've completed our CNN model. Now we will train it by our MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0ji1gcjoGBZ"
   },
   "source": [
    "### 10.Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YR6L0PP3oGBa"
   },
   "source": [
    "#### Define the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkcDi7rwoGBa"
   },
   "source": [
    "1. We use Binary Cross-Entropy function (Log loss) to determine the loss of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYVFTat6oGBa"
   },
   "source": [
    "$$H(p,q) = -\\dfrac{1}{N}\\sum_{x}p(x)\\log(q(x))+(1-p(x))\\log (1-q(x))$$\n",
    "\n",
    "where $p$ is a probability of actual values, $q$ is a probability of predicted values and $N$ is number of instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NODxfcs7oGBb"
   },
   "outputs": [],
   "source": [
    "#binary_cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_act*tf.log(y_CNN)+(1-y_act)*tf.log(1-y_CNN),\n",
    "                 #                                    reduction_indices=[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7lSxlAQoGBd"
   },
   "source": [
    "2. We use cross-entropy function to determine the loss of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtsRTfY_oGBd"
   },
   "source": [
    "$$H(p,q) = -\\dfrac{1}{N}\\sum_{x}p(x)\\log q(x)$$\n",
    "\n",
    "where $p$ is a probability of actual values, $q$ is a probability of predicted values and $N$ is number of instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3_T56ozoGBe"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_act*tf.log(y_CNN),reduction_indices=[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtcLIkZToGBh"
   },
   "source": [
    "#### Define the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OClMKGrpoGBi"
   },
   "source": [
    "We use the optimizer AdamOptimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pb-ZH9VBoGBj"
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(learning_rate = 1e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69H_G2YYoGBk"
   },
   "source": [
    "#### Define prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2VxkSaMoGBm"
   },
   "outputs": [],
   "source": [
    "correct_prediction =  tf.equal(tf.argmax(y_CNN,1),tf.argmax(y_act,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shP_cuePoGBq"
   },
   "source": [
    "#### Define accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xDgQ5fVoGBr"
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWCjcE_koGBu"
   },
   "source": [
    "### 11. Run session and Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Sx30gnooGBv"
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFn91b5roGBx"
   },
   "outputs": [],
   "source": [
    "#Vectors for stroring accuracy and loss values\n",
    "train_accuracy = []\n",
    "train_loss = []\n",
    "test_accuracy = []\n",
    "test_loss = []\n",
    "#Setting batch size to avoid costly computation\n",
    "size_batch = 50\n",
    "train_n_batches = len(X_train[0])//size_batch\n",
    "test_n_batches = len(X_test[0])//size_batch\n",
    "\n",
    "for _ in range(3000):\n",
    "    cumulative_train_accuracy = 0.0\n",
    "    cumulative_train_loss =0.0\n",
    "    cumulative_test_accuracy = 0.0\n",
    "    cumulative_test_loss =0.0\n",
    "    #Train the model\n",
    "    for i in range(train_n_batches):\n",
    "        batch_X = X_train[i*size_batch:(i+1)*size_batch]\n",
    "        batch_y = y_train[i*size_batch:(i+1)*size_batch]\n",
    "        batch_train_accuracy, batch_train_loss = sess.run([accuracy,cross_entropy],feed_dict = {x_input: batch_X,y_act: batch_y,keep_prob: 0.8})\n",
    "        cumulative_train_accuracy += batch_train_accuracy\n",
    "        cumulative_train_loss += batch_train_loss\n",
    "        train_step.run(feed_dict = {x_input: batch_X, y_act:batch_y,keep_prob: 0.5})\n",
    "    train_accuracy.append(cumulative_train_accuracy/train_n_batches)\n",
    "    train_loss.append(cumulative_train_loss/train_n_batches)\n",
    "    #Evaluate the model (evaluate in batches to avoid out-of-memory issues)\n",
    "    for j in range(test_n_batches):\n",
    "        batch_X = X_test[j*size_batch:(j+1)*size_batch]\n",
    "        batch_y = y_test[j*size_batch:(j+1)*size_batch]\n",
    "        batch_test_accuracy, batch_test_loss = sess.run([accuracy,cross_entropy],feed_dict = {x_input:batch_X, y_act:batch_y,keep_prob: 0.8})\n",
    "        cumulative_test_accuracy += batch_test_accuracy\n",
    "        cumulative_test_loss += batch_test_loss\n",
    "    test_accuracy.append(cumulative_test_accuracy/test_n_batches)\n",
    "    test_loss.append(cumulative_test_loss/test_n_batches)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVScte4BoGBy"
   },
   "source": [
    "### 12.Plot train-validation loss and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "5j3UadrAoGBz",
    "outputId": "9dda0d4d-3dff-4447-9ad8-477f951d3399"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHQhJREFUeJzt3XuUVOWd7vHvI/cIgiCK0mIT8SRp\nQDukJMuIxihRcGI0SqKOKIN6yEVjMsTEnpiJBJ1z1DOjiUqOYQxZaKJ4W0YyXjhoRhMlCTSKFySk\nES80QQW8gfeW3/mjNli21XTT/VZXN/181qrFvry19++1Xf30u99duxQRmJmZtdUu5S7AzMx2Dg4U\nMzNLwoFiZmZJOFDMzCwJB4qZmSXhQDEzsyQcKNblSeomabOkYSnbdhaSTpD0ZLnrsM7PgWKdTvYL\nfetri6S3CtZP29HjRcT7EdE3Ip5P2XZHSPofjfoVkt4oWD+4Dcd+VVIuZb1mxXQvdwFmOyoi+m5d\nlvQscHZE3NdUe0ndI6KhPWprrYj4G9AXQFJv4C3gExFRX9bCzHaARyi205F0iaSbJd0kaRMwWdIh\nkv6c/bW+TtJVknpk7btnI4LKbP3X2f57JG2S9CdJw3e0bbZ/oqS/SXpN0tWSHpb0T63s166SZkmq\nz/pwZUEfKiQtzPq3UdI92fbfAf2BP2QjnWktOM+nJS3Kal4maXzBvklZfzZJel7S17d3futaHCi2\ns/oKcCP5X6Y3Aw3Ad4A9gEOBCcDXt/P+fwT+FRgIPA9cvKNtJe0J3AJ8PzvvM8DY1nYIuAbYHajK\nXtXA9GzfhcCjwCBgb+B/AUTEccBrwOHZpbrZ2zuBpF2Bu4HfZDX/K3BHFhjdgF8CX4uIfsBngD9v\n7/zWtThQbGf1UET8LiK2RMRbEbEkIv4SEQ0RsRqYDXx+O++/LSJqI+I98r9cq1vR9kvAsoi4M9t3\nJbChNZ2R1Ac4HTgvIl6PiFeAy4FTsibvAUOBfSPi3Yj4Y2vOAxwJbIqIWRHxXkT8DngImAQE8D4w\nUtKuEbE+Ih5LfH7rxBwotrNaU7gi6ZOS7pL0gqTXgZnk/wJvygsFy2+SzW/sYNt9CuuI/JNYt82J\nSFpZMOl+yHZ7A/sC3YBV2WWlV4F5wJ7Z/hnAK8Afs+N+u5njNWUf4LlG254DhkbEFuB44DRgbXaJ\na2t4pjq/dWIOFNtZNX6M9i+AJ4EREbEb8GNAJa5hHVCxdUWSyP8Vny8w4hPZZai+EfGnZo5VT350\nsG9EDMhe/SNiaHaslyPi3IjYl/wluH+T9Jmtp9qBmv8O7Ndo2zBgbXaeP0bEscBe5EcuN7Tg/NZF\nOFCsq+hHfi7hDUmfYvvzJ6n8FzBG0nGSupOfwxncmgNFxJvkL6f9TNJA5e0n6SjY9lmSyqz5a8CW\n7AXwIvDxFp7q98Bukr6Z3YBwLHAYcLuk/pK+Kqkf8C6waes5mjm/dREOFOsqvgdMIf9L8BfkJ+pL\nKiJeBE4GrgA2AvuTn7h+p5WH/Bb5X9aPAq+SD6zKbN9o8peb3gDuB34SEY9m+y4Grswulf3PZmp+\nA/gH4Iys5v8NnBQRa8iP6L5B/jLeq+TnVc5swfmti5C/YMusfWR3Sf0dmORJa9sZeYRiVkKSJkga\nIKkX+Vtw3wMWl7kss5JwoJiV1jhgNbAeOAb4SkS09pKXWYfmS15mZpaERyhmZpZEl3o45B577BGV\nlZXlLsPMrFNZunTphoho9pb3LhUolZWV1NbWlrsMM7NORVLjpycU5UteZmaWhAPFzMyScKCYmVkS\nXWoOxcw6hvfee4/6+nrefvvtcpdiBXr37k1FRQU9evRo1fsdKGbW7urr6+nXrx+VlZXkH8Js5RYR\nbNy4kfr6eoYPH978G4rwJS8za3dvv/02gwYNcph0IJIYNGhQm0aNDhQzKwuHScfT1p+JA8XMzJJw\noJhZl7Nx40aqq6uprq5myJAhDB06dNv6u+++26JjTJ06lZUrV263zaxZs/jNb36TomTGjRvHsmXL\nkhyrVDwpb2ZdzqBBg7b9cp4xYwZ9+/bl/PPP/1CbiCAi2GWX4n93/+pXv2r2POecc07bi+1EPEIx\nM8usWrWKqqoqTjvtNEaOHMm6deuYNm0auVyOkSNHMnPmzG1tt44YGhoaGDBgADU1NRx00EEccsgh\nvPTSSwD86Ec/4qc//em29jU1NYwdO5ZPfOITLFq0CIA33niDk046iaqqKiZNmkQul2vxSOStt95i\nypQpjB49mjFjxvCHP/wBgCeeeIKDDz6Y6upqDjzwQFavXs2mTZuYOHEiBx10EKNGjeK2225L+Z8O\n8AjFzMrsu9+F1Fdyqqsh+z2+w/76179y/fXXk8vlALj00ksZOHAgDQ0NfOELX2DSpElUVVV96D2v\nvfYan//857n00kuZPn06c+bMoaam5iPHjggWL17M/PnzmTlzJvfeey9XX301Q4YM4fbbb+exxx5j\nzJgxLa71qquuolevXjzxxBMsX76cY489lrq6On7+859z/vnnc/LJJ/POO+8QEdx5551UVlZyzz33\nbKs5NY9QzMwK7L///tvCBOCmm25izJgxjBkzhhUrVvDUU0995D19+vRh4sSJAHzmM5/h2WefLXrs\nE0888SNtHnroIU455RQADjroIEaOHNniWh966CEmT54MwMiRI9lnn31YtWoVn/vc57jkkku4/PLL\nWbNmDb179+bAAw/k3nvvpaamhocffpj+/fu3+Dwt5RGKmZVVa0cSpbLrrrtuW66rq+NnP/sZixcv\nZsCAAUyePLno5zR69uy5bblbt240NDQUPXavXr2abZPC6aefziGHHMJdd93FhAkTmDNnDocffji1\ntbXcfffd1NTUMHHiRH74wx8mPa9HKGZmTXj99dfp168fu+22G+vWrWPBggXJz3HooYdyyy23APm5\nj2IjoKYcdthh2+4iW7FiBevWrWPEiBGsXr2aESNG8J3vfIcvfelLPP7446xdu5a+ffty+umn873v\nfY9HHnkkeV88QjEza8KYMWOoqqrik5/8JPvttx+HHnpo8nN8+9vf5owzzqCqqmrbq6nLUcccc8y2\n52wddthhzJkzh69//euMHj2aHj16cP3119OzZ09uvPFGbrrpJnr06ME+++zDjBkzWLRoETU1Neyy\nyy707NmTa6+9NnlfutR3yudyufAXbJmV34oVK/jUpz5V7jI6hIaGBhoaGujduzd1dXUcffTR1NXV\n0b17ef7eL/azkbQ0InJNvGUbj1DMzMpo8+bNHHXUUTQ0NBAR/OIXvyhbmLRV56zazGwnMWDAAJYu\nXVruMpLwpLyZmSXhQDEzsyQcKGZmloQDxczMknCgmFmXk+Lx9QBz5szhhRdeKLpv8uTJ/Pa3v01V\ncqfgu7zMrMtpyePrW2LOnDmMGTOGIUOGpC6xUyrrCEXSBEkrJa2S9JFHc0rqJenmbP9fJFU22j9M\n0mZJO/5/gplZEXPnzmXs2LFUV1fzrW99iy1bttDQ0MDpp5/O6NGjGTVqFFdddRU333wzy5Yt4+ST\nT27xyGbLli1Mnz6dUaNGMXr06G2PkF+7di3jxo2jurqaUaNGsWjRoqLn7OjKNkKR1A2YBXwRqAeW\nSJofEYUPsjkLeCUiRkg6BbgMOLlg/xXAPe1Vs5mVQAd6fv2TTz7JHXfcwaJFi+jevTvTpk1j3rx5\n7L///mzYsIEnnngCgFdffZUBAwZw9dVXc80111BdXd2i4996662sWLGCxx57jPXr13PwwQdz+OGH\n8+tf/5rjjjuOCy64gPfff5+33nqLpUuXfuScHV05RyhjgVURsToi3gXmAcc3anM8MDdbvg04SpIA\nJJ0APAMsb6d6zWwnd99997FkyRJyuRzV1dU8+OCDPP3004wYMYKVK1dy3nnnsWDBglY/+v2hhx7i\n1FNPpVu3bgwZMoRx48ZRW1vLwQcfzHXXXcdPfvITnnzySfr27ZvsnO2pnHMoQ4E1Bev1wGebahMR\nDZJeAwZJehu4gPzoZruXuyRNA6YBDBs2LE3lZpZOB3p+fURw5plncvHFF39k3+OPP84999zDrFmz\nuP3225k9e3ay8x555JE88MAD3HXXXZxxxhn84Ac/4LTTTivpOUuhs97lNQO4MiI2N9cwImZHRC4i\ncoMHDy59ZWbWaY0fP55bbrmFDRs2APm7wZ5//nnWr19PRPDVr36VmTNnbnv0e79+/di0aVOLj3/Y\nYYcxb948tmzZwosvvsjDDz9MLpfjueeeY8iQIUybNo2pU6fy6KOPNnnOjqycI5S1wL4F6xXZtmJt\n6iV1B/oDG8mPZCZJuhwYAGyR9HZEXFP6ss1sZzV69Gguuugixo8fz5YtW+jRowfXXnst3bp146yz\nziIikMRll10GwNSpUzn77LPp06cPixcv/tAXbQGcffbZnHvuuQAMHz6cBx98kD//+c8ceOCBSOKK\nK65gzz33ZM6cOVxxxRX06NGDfv36ccMNN7BmzZqi5+zIyvb4+iwg/gYcRT44lgD/GBHLC9qcA4yO\niG9kk/InRsTXGh1nBrA5Iv69uXP68fVmHYMfX99xdcrH12dzIucCC4BuwJyIWC5pJlAbEfOBXwI3\nSFoFvAycUq56zcxs+8r6wcaIuBu4u9G2Hxcsvw18tZljzChJcWZmtkM666S8mXVyXenbYjuLtv5M\nHChm1u569+7Nxo0bHSodSESwceNGevfu3epj+FleZtbuKioqqK+vZ/369eUuxQr07t2bioqKVr/f\ngWJm7a5Hjx4MHz683GVYYr7kZWZmSThQzMwsCQeKmZkl4UAxM7MkHChmZpaEA8XMzJJwoJiZWRIO\nFDMzS8KBYmZmSThQzMwsCQeKmZkl4UAxM7MkHChmZpaEA8XMzJJwoJiZWRIOFDMzS8KBYmZmSThQ\nzMwsCQeKmZkl4UAxM7MkHChmZpaEA8XMzJJwoJiZWRIOFDMzS8KBYmZmSThQzMwsibIGiqQJklZK\nWiWppsj+XpJuzvb/RVJltv2LkpZKeiL798j2rt3MzD6sbIEiqRswC5gIVAGnSqpq1Ows4JWIGAFc\nCVyWbd8AHBcRo4EpwA3tU7WZmTWlnCOUscCqiFgdEe8C84DjG7U5HpibLd8GHCVJEfFoRPw9274c\n6COpV7tUbWZmRZUzUIYCawrW67NtRdtERAPwGjCoUZuTgEci4p0S1WlmZi3QvdwFtIWkkeQvgx29\nnTbTgGkAw4YNa6fKzMy6nnKOUNYC+xasV2TbiraR1B3oD2zM1iuAO4AzIuLppk4SEbMjIhcRucGD\nBycs38zMCpUzUJYAB0gaLqkncAowv1Gb+eQn3QEmAb+PiJA0ALgLqImIh9utYjMza1LZAiWbEzkX\nWACsAG6JiOWSZkr6ctbsl8AgSauA6cDWW4vPBUYAP5a0LHvt2c5dMDOzAoqIctfQbnK5XNTW1pa7\nDDOzTkXS0ojINdfOn5Q3M7MkHChmZpaEA8XMzJJwoJiZWRIOFDMzS8KBYmZmSThQzMwsCQeKmZkl\n4UAxM7MkHChmZpaEA8XMzJJwoJiZWRIOFDMzS8KBYmZmSThQzMwsCQeKmZkl4UAxM7MkHChmZpaE\nA8XMzJJwoJiZWRIOFDMzS8KBYmZmSThQzMwsCQeKmZkl4UAxM7MkHChmZpZEiwJF0v6SemXLR0g6\nT9KA0pZmZmadSUtHKLcD70saAcwG9gVuLFlVZmbW6bQ0ULZERAPwFeDqiPg+sHfpyjIzs86mpYHy\nnqRTgSnAf2XbepSmJDMz64xaGihTgUOAf4uIZyQNB24oXVlmZtbZtChQIuKpiDgvIm6StDvQLyIu\na+vJJU2QtFLSKkk1Rfb3knRztv8vkioL9v1Ltn2lpGPaWouZmbVNS+/yekDSbpIGAo8A/ynpirac\nWFI3YBYwEagCTpVU1ajZWcArETECuBK4LHtvFXAKMBKYAPw8O56ZmZVJSy959Y+I14ETgesj4rPA\n+DaeeyywKiJWR8S7wDzg+EZtjgfmZsu3AUdJUrZ9XkS8ExHPAKuy45mZWZm0NFC6S9ob+BofTMq3\n1VBgTcF6fbataJvsLrPXgEEtfC8AkqZJqpVUu379+kSlm5lZYy0NlJnAAuDpiFgi6eNAXenKSici\nZkdELiJygwcPLnc5ZmY7re4taRQRtwK3FqyvBk5q47nXkv+A5FYV2bZibeoldQf6Axtb+F4zM2tH\nLZ2Ur5B0h6SXstftkiraeO4lwAGShkvqSX6SfX6jNvPJf/YFYBLw+4iIbPsp2V1gw4EDgMVtrMfM\nzNqgpZe8fkX+l/g+2et32bZWy+ZEziV/KW0FcEtELJc0U9KXs2a/BAZJWgVMB2qy9y4HbgGeAu4F\nzomI99tSj5mZtY3yf/A300haFhHVzW3r6HK5XNTW1pa7DDOzTkXS0ojINdeupSOUjZImS+qWvSaT\nn8swMzMDWh4oZ5K/ZfgFYB35+Yx/KlFNZmbWCbX00SvPRcSXI2JwROwZESfQ9ru8zMxsJ9KWb2yc\nnqwKMzPr9NoSKEpWhZmZdXptCZTmbw8zM7MuY7uflJe0ieLBIaBPSSoyM7NOabuBEhH92qsQMzPr\n3NpyycvMzGwbB4qZmSXhQDEzsyQcKGZmloQDxczMknCgmJlZEg4UMzNLwoFiZmZJOFDMzCwJB4qZ\nmSXhQDEzsyQcKGZmloQDxczMknCgmJlZEg4UMzNLwoFiZmZJOFDMzCwJB4qZmSXhQDEzsyQcKGZm\nloQDxczMknCgmJlZEmUJFEkDJS2UVJf9u3sT7aZkbeokTcm2fUzSXZL+Kmm5pEvbt3ozMyumXCOU\nGuD+iDgAuD9b/xBJA4GLgM8CY4GLCoLn3yPik8CngUMlTWyfss3MrCnlCpTjgbnZ8lzghCJtjgEW\nRsTLEfEKsBCYEBFvRsR/A0TEu8AjQEU71GxmZttRrkDZKyLWZcsvAHsVaTMUWFOwXp9t20bSAOA4\n8qMcMzMro+6lOrCk+4AhRXZdWLgSESEpWnH87sBNwFURsXo77aYB0wCGDRu2o6cxM7MWKlmgRMT4\npvZJelHS3hGxTtLewEtFmq0FjihYrwAeKFifDdRFxE+bqWN21pZcLrfDwWVmZi1Trkte84Ep2fIU\n4M4ibRYAR0vaPZuMPzrbhqRLgP7Ad9uhVjMza4FyBcqlwBcl1QHjs3Uk5SRdBxARLwMXA0uy18yI\neFlSBfnLZlXAI5KWSTq7HJ0wM7MPKKLrXAXK5XJRW1tb7jLMzDoVSUsjItdcO39S3szMknCgmJlZ\nEg4UMzNLwoFiZmZJOFDMzCwJB4qZmSXhQDEzsyQcKGZmloQDxczMknCgmJlZEg4UMzNLwoFiZmZJ\nOFDMzCwJB4qZmSXhQDEzsyQcKGZmloQDxczMknCgmJlZEg4UMzNLwoFiZmZJOFDMzCwJB4qZmSXh\nQDEzsyQcKGZmloQDxczMknCgmJlZEg4UMzNLwoFiZmZJOFDMzCwJB4qZmSXhQDEzsyTKEiiSBkpa\nKKku+3f3JtpNydrUSZpSZP98SU+WvmIzM2tOuUYoNcD9EXEAcH+2/iGSBgIXAZ8FxgIXFQaPpBOB\nze1TrpmZNadcgXI8MDdbngucUKTNMcDCiHg5Il4BFgITACT1BaYDl7RDrWZm1gLlCpS9ImJdtvwC\nsFeRNkOBNQXr9dk2gIuB/wDebO5EkqZJqpVUu379+jaUbGZm29O9VAeWdB8wpMiuCwtXIiIkxQ4c\ntxrYPyL+WVJlc+0jYjYwGyCXy7X4PGZmtmNKFigRMb6pfZJelLR3RKyTtDfwUpFma4EjCtYrgAeA\nQ4CcpGfJ17+npAci4gjMzKxsynXJaz6w9a6tKcCdRdosAI6WtHs2GX80sCAi/m9E7BMRlcA44G8O\nEzOz8itXoFwKfFFSHTA+W0dSTtJ1ABHxMvm5kiXZa2a2zczMOiBFdJ1phVwuF7W1teUuw8ysU5G0\nNCJyzbXzJ+XNzCwJB4qZmSXhQDEzsyQcKGZmloQDxczMknCgmJlZEg4UMzNLwoFiZmZJOFDMzCwJ\nB4qZmSXhQDEzsyQcKGZmloQDxczMknCgmJlZEg4UMzNLwoFiZmZJOFDMzCwJB4qZmSXhQDEzsyQc\nKGZmloQDxczMknCgmJlZEg4UMzNLwoFiZmZJKCLKXUO7kbQeeK7cdeygPYAN5S6inbnPXYP73Hns\nFxGDm2vUpQKlM5JUGxG5ctfRntznrsF93vn4kpeZmSXhQDEzsyQcKB3f7HIXUAbuc9fgPu9kPIdi\nZmZJeIRiZmZJOFDMzCwJB0oHIGmgpIWS6rJ/d2+i3ZSsTZ2kKUX2z5f0ZOkrbru29FnSxyTdJemv\nkpZLurR9q98xkiZIWilplaSaIvt7Sbo52/8XSZUF+/4l275S0jHtWXdbtLbPkr4oaamkJ7J/j2zv\n2lujLT/jbP8wSZslnd9eNZdERPhV5hdwOVCTLdcAlxVpMxBYnf27e7a8e8H+E4EbgSfL3Z9S9xn4\nGPCFrE1P4I/AxHL3qYl+dgOeBj6e1foYUNWozbeAa7PlU4Cbs+WqrH0vYHh2nG7l7lOJ+/xpYJ9s\neRSwttz9KWV/C/bfBtwKnF/u/rTl5RFKx3A8MDdbngucUKTNMcDCiHg5Il4BFgITACT1BaYDl7RD\nram0us8R8WZE/DdARLwLPAJUtEPNrTEWWBURq7Na55Hve6HC/xa3AUdJUrZ9XkS8ExHPAKuy43V0\nre5zRDwaEX/Pti8H+kjq1S5Vt15bfsZIOgF4hnx/OzUHSsewV0Ssy5ZfAPYq0mYosKZgvT7bBnAx\n8B/AmyWrML229hkASQOA44D7S1FkAs32obBNRDQArwGDWvjejqgtfS50EvBIRLxTojpTaXV/sz8G\nLwB+0g51llz3chfQVUi6DxhSZNeFhSsREZJafC+3pGpg/4j458bXZcutVH0uOH534CbgqohY3boq\nrSOSNBK4DDi63LWU2AzgyojYnA1YOjUHSjuJiPFN7ZP0oqS9I2KdpL2Bl4o0WwscUbBeATwAHALk\nJD1L/ue5p6QHIuIIyqyEfd5qNlAXET9NUG6prAX2LVivyLYVa1OfhWR/YGML39sRtaXPSKoA7gDO\niIinS19um7Wlv58FJkm6HBgAbJH0dkRcU/qyS6Dckzh+BcD/4cMT1JcXaTOQ/HXW3bPXM8DARm0q\n6TyT8m3qM/n5otuBXcrdl2b62Z38zQTD+WDCdmSjNufw4QnbW7LlkXx4Un41nWNSvi19HpC1P7Hc\n/WiP/jZqM4NOPilf9gL8CshfO74fqAPuK/ilmQOuK2h3JvmJ2VXA1CLH6UyB0uo+k/8LMIAVwLLs\ndXa5+7Sdvh4L/I38nUAXZttmAl/OlnuTv8NnFbAY+HjBey/M3reSDnonW8o+Az8C3ij4uS4D9ix3\nf0r5My44RqcPFD96xczMkvBdXmZmloQDxczMknCgmJlZEg4UMzNLwoFiZmZJOFDM2kjS+5KWFbw+\n8rTZNhy7srM8QdrMn5Q3a7u3IqK63EWYlZtHKGYlIulZSZdn3+2xWNKIbHulpN9LelzS/ZKGZdv3\nknSHpMey1+eyQ3WT9J/Zd7/8P0l9svbnSXoqO868MnXTbBsHilnb9Wl0yevkgn2vRcRo4Bpg6zPH\nrgbmRsSBwG+Aq7LtVwEPRsRBwBg+eJz5AcCsiBgJvEr+KbyQf2TNp7PjfKNUnTNrKX9S3qyNJG2O\niL5Ftj8LHBkRqyX1AF6IiEGSNgB7R8R72fZ1EbGHpPVARRQ8rj17gvTCiDggW78A6BERl0i6F9gM\n/Bb4bURsLnFXzbbLIxSz0oomlndE4feBvM8Hc5//AMwiP5pZkj3F1qxsHChmpXVywb9/ypYXkX/i\nLMBp5L/CGPIPy/wmgKRukvo3dVBJuwD7Rv6bKy8g/zj0j4ySzNqT/6Ixa7s+kpYVrN8bEVtvHd5d\n0uPkRxmnZtu+DfxK0veB9cDUbPt3gNmSziI/EvkmsI7iugG/zkJH5L9k7NVkPTJrBc+hmJVINoeS\ni4gN5a7FrD34kpeZmSXhEYqZmSXhEYqZmSXhQDEzsyQcKGZmloQDxczMknCgmJlZEv8flxgjlf1p\nB7AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Training-Validation loss\n",
    "plt.plot(range(len(train_loss)),train_loss, color = 'b', label = \"Training Loss\" )\n",
    "plt.plot(range(len(train_loss)),test_loss, color = 'r', label = \"Test Loss\" )\n",
    "plt.title(\"Training-Test loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "8EFBizU8oGB2",
    "outputId": "5a4501eb-cd1c-43e8-a9be-d737f993c3a1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Training-Validation Accuracy\n",
    "plt.plot(range(len(train_accuracy)),train_accuracy, color = 'b', label = \"Training accuracy\" )\n",
    "plt.plot(range(len(train_accuracy)),test_accuracy, color = 'r', label = \"Test accuracy\" )\n",
    "plt.title(\"Training-Test accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYrfVKOWoGB6"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RbWsDxFoGB6"
   },
   "outputs": [],
   "source": [
    "#evaluate in batches to avoid out-of-memory issues\n",
    "n_batches = mnist.test.images.shape[0]//50\n",
    "cumulative_accuracy = 0.0\n",
    "for index in range(n_batches):\n",
    "    batch = mnist.test.next_batch(50)\n",
    "    cumulative_accuracy += accuracy.eval(feed_dict = {x:batch[0], y_act:batch[1],keep_prob: 1.0})\n",
    "print(\"Test accuracy {}\".format(cumulative_accuracy/n_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXiSG2V2oGB8"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYL0_NpboGB8"
   },
   "source": [
    "Fistly, we will visualize all the filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpVfk20FoGB8"
   },
   "outputs": [],
   "source": [
    "kernels = sess.run(tf.reshape(tf.transpose(W_conv1, perm=[2, 3, 0,1]),[32, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93kFaFXmoGB-"
   },
   "outputs": [],
   "source": [
    "!wget --output-document utils1.py http://deeplearning.net/tutorial/code/utils.py\n",
    "import utils1\n",
    "from utils1 import tile_raster_images\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "image = Image.fromarray(tile_raster_images(kernels, img_shape=(5, 5) ,tile_shape=(4, 8), tile_spacing=(1, 1)))\n",
    "### Plot image\n",
    "plt.rcParams['figure.figsize'] = (18.0, 18.0)\n",
    "imgplot = plt.imshow(image)\n",
    "imgplot.set_cmap('gray')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4WJmnLGoGB_"
   },
   "source": [
    "Secondly,  we will see the output of an image passing through 1st convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKsm9jL-oGCA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = (5.0,5.0)\n",
    "sampleimage = mnist.test.images[1]\n",
    "plt.imshow(np.reshape(sampleimage,[28,28]),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRJDKVqqoGCC"
   },
   "outputs": [],
   "source": [
    "ActivatedUnits = sess.run(convolve1,feed_dict={x:np.reshape(sampleimage,[1,784],order='F'),keep_prob:1.0})\n",
    "filters = ActivatedUnits.shape[3] #remember that size of 1st convolution layer is\n",
    "                                  # [-1,28,28,32]      \n",
    "plt.figure(1, figsize=(20,20))\n",
    "n_columns = 6\n",
    "n_rows = np.math.ceil(filters / n_columns) + 1\n",
    "for i in range(filters):\n",
    "    plt.subplot(n_rows, n_columns, i+1)\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    plt.title('Image through filter ' + str(i))\n",
    "    plt.imshow(ActivatedUnits[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wn1EIsIdoGCE"
   },
   "source": [
    "Finally, we will see the output of an image passing through 2nd convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3rOVYFQRoGCG"
   },
   "outputs": [],
   "source": [
    "ActivatedUnits = sess.run(convolve2,feed_dict={x:np.reshape(sampleimage,[1,784],order='F'),keep_prob:1.0})\n",
    "filters = ActivatedUnits.shape[3]#size of 2nd convolution layer is\n",
    "                                  # [-1,7,7,64]\n",
    "plt.figure(1, figsize=(20,20))\n",
    "n_columns = 8\n",
    "n_rows = np.math.ceil(filters / n_columns) + 1\n",
    "for i in range(filters):\n",
    "    plt.subplot(n_rows, n_columns, i+1)\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    plt.title('Image through filter ' + str(i))\n",
    "    plt.imshow(ActivatedUnits[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN-cat-dog-classification.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
